
from pandas import read_csv, isnull
from os import path
from hashlib import sha256
from Bio import SeqIO
from collections import defaultdict

genomes,       = glob_wildcards("genomes/{genome}.fna")
algal_genomes, = glob_wildcards("genomes_algae/{genome}.fna")
hmms,          = glob_wildcards("hmm/{hmm}.hmm")
hmm_algae,     = glob_wildcards("hmm_algae/{hmm}.hmm")
capsid_proteins = defaultdict(list)

with open("metadata/capsid_proteins.txt") as fh:
    for line in fh:
        hmm, CP = line.rstrip().split()
        capsid_proteins[CP].append(hmm)

segments_file = 'metadata/viral_segments.tsv'
viruses_file  = 'metadata/viruses.tsv'

evalues = { "MCP_PLV": 1e-8, "MCP_NCLDV": 1e-8 }

# search = [ "autoblast", "blastp", "diamond", "usearch", "ublast", "lastp", "rapsearch", "topaz", "blatp", "mmseqsp" ]
search = [ "diamond" ]

segments = {}
PgVV_segments = []
data = read_csv(segments_file, sep = '\t')
for i, row in data.iterrows():
    if isnull(row['fragment']):
        short = row['short']
        segments[short] = row.to_dict()
        if row['clade'] == 'PgVV':
            PgVV_segments.append(short)
segment_names = list(segments.keys())

viruses = {}
small_viruses = []
PgVV_viruses = []
Mesomimi_viruses = []
data = read_csv(viruses_file, sep = '\t')
for i, row in data.iterrows():
    if isnull(row['fragment']):
        short = row['short']
        viruses[short] = row.to_dict()
        if row['clade'] == 'PgVV':
            PgVV_viruses.append(short)
        if row['clade'] == 'Mesomimi':
            Mesomimi_viruses.append(short)
        if row['group'] == 'small':
            small_viruses.append(short)
virus_names = list(viruses.keys())

def orthology():
    prefix = []
    for short, row in viruses.items():
        if row['clade'] == "Mesomimi" or row['group'] == "small":
            prefix.append(short)
    return prefix

rule all:
    input:
        expand("analysis/PLV_segments_hh/{segment}-hhsearch-pfam.tsv", segment = segment_names),
        expand("analysis/PLVs_hh/{segment}-hhsearch-pfam.tsv", segment = small_viruses + Mesomimi_viruses),
        "analysis/hh_mcl/mcl.txt",
        # expand("analysis/PLVs/orthogroups/{search}", search = search),
        # expand("analysis/hmmsearch/{genome}-{hmm}.txt", genome = genomes, hmm = hmms),
        # expand("analysis/hmmsearch/{genome}-{hmm}.faa", genome = genomes, hmm = hmms),
        expand("analysis/segments/{genome}/segments.gbk", genome = genomes),
        "analysis/vcontact2/results",
        # expand("analysis/blast_algae/{genome}.blast", genome = algal_genomes),
        # expand("analysis/phylogeny/{CP}.svg", CP = [ "MCP_NCLDV" ]),
        "analysis/promoters.svg",
        "output/vcontact2_clustering.svg"
        # "analysis/proteinortho/viruses.proteinortho.tsv",

rule viruses_ncbi_fna:
    output:
        "analysis/PLV_annotate/prokka-{genome}.fna"
    params:
        id = lambda w: viruses[w.genome]['accession']
    resources:
        ncbi = 1
    conda:
        "envs/tools.yaml"
    shell:
        "efetch -db nuccore -id {params.id} -format fasta > {output}"

rule viruses_ncbi_gbk:
    output:
        "analysis/PLV_annotate/ncbi-{genome}.gbk"
    params:
        id = lambda w: viruses[w.genome]['accession']
    resources:
        ncbi = 1
    conda:
        "envs/tools.yaml"
    shell:
        "efetch -db nuccore -id {params.id} -format gb > {output}"

rule extract_segment:
    input:
        lambda w: expand("analysis/segments/{host}/segments.gbk", host = segments[w.segment]['genome'])
    output:
        "analysis/PLV_segments/{segment}.gbk"
    params:
        flank = 50000,
        segment = lambda w: segments[w.segment]
    conda:
        "envs/biopython.yaml"
    script:
        "scripts/extract_segment.py"

rule mv_faa:
    input:
        lambda w: expand("analysis/PLV_annotate/{annotations}-{genome}.faa", annotations = viruses[w.genome]['annotations'], genome = w.genome)
    output:
        "analysis/PLVs/{genome}.faa"
    shell:
        "mv {input} {output}"

rule prokka_gbk:
    input:
        "analysis/PLV_annotate/prokka-{genome}.fna"
    output:
        "analysis/PLV_annotate/prokka-{genome}.gbk"
    params:
        outdir = "analysis/PLVs/ncbi_annotate/",
        locustag = lambda w: re.sub('[^A-Z]', '', w.genome.upper())
    threads:
        8
    conda:
        "envs/prokka.yaml"
    shell:
        """
        prokka --force --cpus {threads} --prefix {wildcards.genome} --locustag {params.locustag} --kingdom Viruses --outdir {params.outdir}/{wildcards.genome} {input}
        mv {params.outdir}/{wildcards.genome}/{wildcards.genome}.gbk {params.outdir}/
        """

rule flanks:
    input:
        "analysis/PLV_annotate/{genome}.gbk"
    output:
        "analysis/promoters/PLVs/{genome}.fna"
    params:
        min_len = 100,
        max_len = 150
    conda:
        "envs/bioperl.yaml"
    shell:
        "perl workflow/helpers/biotags.pl -i {input} -p CDS -t seq-{params.max_len} | awk '$1' | nl -w1 | seqkit tab2fx | seqkit seq -gm{params.min_len} -o {output}"

rule meme:
    input:
        "analysis/promoters/PLVs/{source}/{genome}.fna"
    output:
        "analysis/promoters/PLVs/{source}/{genome}/meme/meme.xml"
    params:
        outdir = "analysis/promoters/PLVs/{source}/{genome}/meme/",
        minw = 6,
        maxw = 16,
        nmotifs = 10,
        minsites = 10
    conda:
        "envs/tools.yaml"
    shell:
        "meme -minw {params.minw} -maxw {params.maxw} -nmotifs {params.nmotifs} -minsites {params.minsites} -maxsites Inf -dna -oc {params.outdir} {input}"

rule plot_meme:
    input:
        "analysis/promoters/PLVs/{source}/{genome}/meme/meme.xml"
    output:
        "analysis/promoters/PLVs/{source}/{genome}/meme/meme.svg"
    params:
        motif_res = [ "[ATW][ATW][ATW][ATW][ATW]TG[ATW]", "TCCGGA" ],
        meme_name = "{genome}"
    conda:
        "envs/r.yaml"
    script:
        "scripts/meme.R"

rule merge_meme:
    input:
        expand("analysis/promoters/PLVs/{virus}/meme/meme.svg", virus = Mesomimi_viruses)
    output:
        "analysis/promoters.svg"
    shell:
        "python workflow/scripts/svg_stack.py --direction=v {input} > {output}"

rule extract_faa_bellas:
    input:
        "databases/All_proteins_Sommaruga/input/All_proteins.faa"
    output:
        "analysis/PLV_annotate/bellas-{segment}.faa"
    params:
        search = lambda w: viruses[w.segment]['search']
    conda:
        "envs/tools.yaml"
    shell:
        "seqkit grep -rp {params.search} {input} | seqkit replace -p '$' -r ' {wildcards.segment}' -o {output}"

rule extract_gbk_yutin:
    input:
        "databases/Yutin2015.gbk"
    output:
        "analysis/PLV_annotate/yutin-{segment}.gbk"
    conda:
        "envs/tools.yaml"
    shell:
        "awk -vl={wildcards.segment} '/^LOCUS/{{p=$2==l}}p' {input} | sed -E 's/\\x93|\\x94/\"/g' > {output}"

rule extract_faa_yutin:
    input:
        "analysis/PLV_annotate/yutin-{segment}.gbk"
    output:
        "analysis/PLV_annotate/yutin-{segment}.faa"
    conda:
        "envs/bioperl.yaml"
    shell:
        "perl workflow/helpers/biotags.pl -i {input} -p CDS -T id -t locus_tag,translation | awk '{{printf\"%s %s\\t%s\\n\",$2,$1,$3}}' | seqkit tab2fx -o {output}"

rule extract_faa_ncbi_annotate:
    input:
        "analysis/PLV_annotate/prokka-{segment}.gbk"
    output:
        "analysis/PLV_annotate/prokka-{segment}.faa"
    conda:
        "envs/bioperl.yaml"
    shell:
        "perl workflow/helpers/biotags.pl -i {input} -p CDS -T description -t locus_tag,translation | awk -F\\\\t '$2{{printf\"%s %s\\t%s\\n\",$2,$1,$3}}' | seqkit tab2fx -o {output}"

rule extract_faa_ncbi:
    input:
        "analysis/PLV_annotate/ncbi-{segment}.gbk"
    output:
        "analysis/PLV_annotate/ncbi-{segment}.faa"
    conda:
        "envs/bioperl.yaml"
    shell:
        "perl workflow/helpers/biotags.pl -i {input} -p CDS -T description -t protein_id,translation | awk -F\\\\t '$2{{printf\"%s %s\\t%s\\n\",$2,$1,$3}}' | seqkit tab2fx -o {output}"

rule extract_faa_segments:
    input:
        "analysis/PLV_segments/{segment}.gbk"
    output:
        "analysis/PLV_segments/{segment}.faa"
    conda:
        "envs/bioperl.yaml"
    shell:
        "perl workflow/helpers/biotags.pl -i {input} -p CDS -t locus_tag,translation | awk '$1' | seqkit tab2fx -o {output}"

rule proteinortho:
    input:
        expand("analysis/PLV_segments/{segment}.faa", segment = segments.keys()),
        expand("analysis/PLVs/{prefix}.faa", prefix = orthology())
    output:
        "analysis/proteinortho/viruses.proteinortho.tsv"
    shadow:
        "shallow"
    params:
        evalue = 1e-5,
        conn   = 0.1,
        outdir = "analysis/proteinortho/",
        project = "viruses",
        search = "diamond"
    threads:
        workflow.cores
    conda:
        "envs/proteinortho.yaml"
    shell:
        """
        proteinortho -project={params.project} -p={params.search} -cpus={threads} -conn={params.conn} -e={params.evalue} {input}
        mv {params.project}.* {params.outdir}
        """

checkpoint proteinortho_collect:
    input:
        "analysis/PLVs/proteinortho/{search}/proteinortho.proteinortho.tsv",
        expand("analysis/PLVs/{genome}.faa", genome = virus_names)
    output:
        directory("analysis/PLVs/orthogroups/{search}")
    shadow:
        "shallow"
    conda:
        "envs/bioperl.yaml"
    shell:
        "perl workflow/helpers/proteinortho_grab_proteins.pl -t -s -output_dir={output} -output_file=sequences.faa -exact {input}"

rule getorf:
    input:
        "genomes/{genome}.fna"
    output:
        "analysis/getorf/{genome}.faa"
    conda:
        "envs/emboss.yaml"
    shell:
        "getorf -minsize 100 -filter {input} > {output}"

rule hmmsearch:
    input:
        fasta = "analysis/getorf/{genome}.faa", hmm = "hmm/{hmm}.hmm"
    output:
        "analysis/hmmsearch/{genome}-{hmm}.txt"
    conda:
        "envs/tools.yaml"
    shell:
        "hmmsearch -o /dev/null --tblout {output} {input.hmm} {input.fasta}"

rule extract_hmmsearch:
    input:
        fasta = "analysis/getorf/{genome}.faa",
        fai = "analysis/getorf/{genome}.faa.seqkit.fai",
        txt = "analysis/hmmsearch/{genome}-{hmm}.txt"
    output:
        "analysis/hmmsearch/{genome}-{hmm}.faa"
    conda:
        "envs/tools.yaml"
    shell:
        """
        awk '!/^#/&&$5<0.001{{print $1,$3}}' OFS=\\\\t {input.txt} | \
            parallel --colsep \\\\t seqkit faidx -f {input.fasta} {{1}} \| seqkit replace -p "'$'" -r '" {{2}}"' | seqkit  seqkit seq -gm 100
        """

rule algae_getorf:
    input:
        "genomes_algae/{genome}.fna"
    output:
        "analysis/getorf_algae/{genome}.faa"
    params:
        minsize = 100,
        maxsize = 100000
    conda:
        "envs/emboss.yaml"
    shell:
        "getorf -minsize {params.minsize} -maxsize {params.maxsize} -filter {input} > {output}"

rule makeblastdb_prot:
    input:
        "{fasta}"
    output:
        "{fasta}.pdb"
    conda:
        "envs/tools.yaml"
    shell:
        "makeblastdb -in {input} -dbtype prot"

rule makeblastdb_nucl:
    input:
        "{fasta}"
    output:
        "{fasta}.ndb"
    conda:
        "envs/tools.yaml"
    shell:
        "makeblastdb -in {input} -dbtype nucl"

rule extract_core_genes:
    input:
        tblout = "analysis/segments/{genome}/segments-hmm-{hmm}.tblout",
        fasta = "analysis/segments/{genome}/segments.faa",
        faidx = "analysis/segments/{genome}/segments.faa.fai"
    output:
        "analysis/CPs/core/{genome}-{hmm}.faa"
    params:
        evalue = 1e-5
    conda:
        "envs/tools.yaml"
    shell:
        "grep -v '^#' {input.tblout} | awk -ve={params.evalue} '$5<e' | cut -f1 -d' ' | xargs seqkit faidx -f {input.fasta} > {output}"

#rule combine_core_genes:
#    input:
#        fasta = lambda w: expand("analysis/CPs/core/{genome}-{hmm}.faa", genome = core_genomes, hmm = capsid_proteins[w.CP])
#    output:
#        "analysis/CPs/queries/{CP}.faa"
#    shell:
#        "cat {input} > {output}"

rule cluster_core_genes:
    input:
        "analysis/CPs/queries/{CP}.faa"
    output:
        "analysis/CPs/queries/{CP}.cdhit"
    params:
        c = 0.9
    conda:
        "envs/tools.yaml"
    shell:
        "cdhit -i {input} -o {output} -c {params.c} -d 0"

rule CP_align:
    input:
        "analysis/CPs/queries/{CP}.faa"
    output:
        "analysis/CPs/queries/{CP}.align"
    conda:
        "envs/tools.yaml"
    shell:
        "mafft {input} > {output}"

rule CP_hmmbuild:
    input:
        "analysis/CPs/queries/{CP}.align"
    output:
        "analysis/CPs/queries/{CP}.hmm"
    conda:
        "envs/tools.yaml"
    shell:
        "hmmbuild {output} {input}"

rule algae_hmmsearch:
    input:
        faa = "analysis/getorf_algae/{genome}.faa",
        hmm = "analysis/CPs/queries/{CP}.hmm"
    output:
        "analysis/phylogeny/algae/hmm/{genome}-{CP}.tblout"
    conda:
        "envs/tools.yaml"
    shell:
        "hmmsearch -o /dev/null --tblout {output} {input.hmm} {input.faa}"

rule PLV_hmmsearch:
    input:
        faa = "analysis/PLVs/{source}/{genome}.faa",
        hmm = "analysis/CPs/queries/{CP}.hmm"
    output:
        "analysis/phylogeny/PLVs/hmm/{source}/{genome}-{CP}.tblout"
    conda:
        "envs/tools.yaml"
    shell:
        "hmmsearch -o /dev/null --tblout {output} {input.hmm} {input.faa}"

rule algae_blast:
    input:
        faa = "analysis/getorf_algae/{genome}.faa",
        pdb = "analysis/getorf_algae/{genome}.faa.pdb",
        query = "analysis/CPs/queries/{CP}.faa"
    output:
        "analysis/phylogeny/algae/blast/{genome}-{CP}.blast"
    params:
        headers = "qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore stitle"
    conda:
        "envs/tools.yaml"
    shell:
        "blastp -query {input.query} -db {input.faa} -outfmt '6 {params.headers}' -out {output}"

rule PLV_blast:
    input:
        faa = "analysis/PLVs/{source}/{genome}.faa",
        pdb = "analysis/PLVs/{source}/{genome}.faa.pdb",
        query = "analysis/CPs/queries/{CP}.faa"
    output:
        "analysis/phylogeny/PLVs/blast/{source}/{genome}-{CP}.blast"
    params:
        headers = "qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore stitle"
    conda:
        "envs/tools.yaml"
    shell:
        "blastp -query {input.query} -db {input.faa} -outfmt '6 {params.headers}' -out {output}"

rule faidx:
    input:
        "{fasta}"
    output:
        "{fasta}.fai"
    conda:
        "envs/tools.yaml"
    shell:
        "seqkit faidx {input}"

rule faidx_f:
    input:
        "{fasta}"
    output:
        "{fasta}.seqkit.fai"
    conda:
        "envs/tools.yaml"
    shell:
        "seqkit faidx -f {input}"

rule hmmsearch_extract_algae:
    input:
        fasta = "analysis/getorf_algae/{genome}.faa",
        fai = "analysis/getorf_algae/{genome}.faa.seqkit.fai",
        tblout = "analysis/phylogeny/algae/hmm/{genome}-{CP}.tblout"
    output:
        "analysis/phylogeny/algae/hmm/{genome}-{CP}.faa"
    params:
        evalue = lambda w: evalues[w.CP]
    conda:
        "envs/tools.yaml"
    shell:
        "awk -ve={params.evalue} '$5<e' {input.tblout} | grep -v '^#' | cut -f1 -d' ' | sort -u | xargs -r seqkit faidx -f {input.fasta} > {output}"

rule hmmsearch_extract_PLV:
    input:
        fasta = "analysis/PLVs/{source}/{genome}.faa",
        fai = "analysis/PLVs/{source}/{genome}.faa.seqkit.fai",
        tblout = "analysis/phylogeny/PLVs/hmm/{source}/{genome}-{CP}.tblout"
    output:
        "analysis/phylogeny/PLVs/hmm/{source}/{genome}-{CP}.faa"
    params:
        evalue = lambda w: evalues[w.CP]
    conda:
        "envs/tools.yaml"
    shell:
        "awk -ve={params.evalue} '$5<e' {input.tblout} | grep -v '^#' | cut -f1 -d' ' | sort -u | xargs -r seqkit faidx -f {input.fasta} > {output}"

rule mad:
    input:
        "{treefile}"
    output:
        "{treefile}.rooted"
    shell:
        "mad {input}"

rule blast_extract_algae:
    input:
        fasta = "analysis/getorf_algae/{genome}.faa",
        fai = "analysis/getorf_algae/{genome}.faa.seqkit.fai",
        blast = "analysis/phylogeny/algae/blast/{genome}-{CP}.blast"
    output:
        "analysis/phylogeny/algae/blast/{genome}-{CP}.faa"
    params:
        evalue = 1e-10
    shell:
        "awk -ve={params.evalue} '$11<e' {input.blast} | cut -f2 | sort -u | xargs -r seqkit faidx -f {input.fasta} > {output}"

rule blast_extract_PLVs:
    input:
        fasta = "analysis/PLVs/{source}/{genome}.faa",
        fai = "analysis/PLVs/{source}/{genome}.faa.seqkit.fai",
        blast = "analysis/phylogeny/PLVs/blast/{source}/{genome}-{CP}.blast"
    output:
        "analysis/phylogeny/PLVs/blast/{source}/{genome}-{CP}.faa"
    params:
        evalue = 1e-10
    conda:
        "envs/tools.yaml"
    shell:
        "awk -ve={params.evalue} '$11<e' {input.blast} | cut -f2 | sort -u | xargs -r seqkit faidx -f {input.fasta} > {output}"

rule algae_hmm:
    input:
        fasta = "analysis/getorf_algae/{genome}.faa",
        hmm = "hmm_algae/{hmm}.hmm"
    output:
        "analysis/hmm_algae/{genome}-{hmm}.tblout"
    conda:
        "envs/tools.yaml"
    shell:
        "hmmsearch -o /dev/null --tblout {output} {input.hmm} {input.fasta}"

#rule blast_extract_cat:
#    input:
#        expand("analysis/phylogeny/algae/blast/{genome}-MCP_NCLDV.faa", genome = algal_genomes),
#        expand("analysis/phylogeny/PLVs/blast/{source}/{genome}-MCP_NCLDV.faa", zip, source = all_sources, genome = all_genomes)
#    output:
#        "analysis/phylogeny/MCP_NCLDV.fasta"
#    shell:
#        "seqkit seq -gm200 {input} -o {output}"

rule hmm_extract_cat:
    input:
        expand("analysis/phylogeny/algae/hmm/{genome}-{{CP}}.faa", genome = algal_genomes),
        expand("analysis/phylogeny/PLVs/hmm/{genome}-{{CP}}.faa", genome = virus_names)
    output:
        "analysis/phylogeny/{CP}.fasta"
    conda:
        "tools.yaml"
    shell:
        "seqkit seq -gm200 {input} -o {output}"

rule fasta_mafft:
    input:
        "{prefix}.fasta"
    output:
        "{prefix}.mafft"
    threads:
        20
    conda:
        "envs/tools.yaml"
    shell:
        "mafft --localpair --maxiterate 1000 --thread {threads} {input} > {output}"

rule fasta_hmmalign:
    input:
        fasta = "analysis/phylogeny/{profile}.fasta",
        hmm = "analysis/CPs/queries/{profile}.hmm"
    output:
        "analysis/phylogeny/{profile}.a2m"
    conda:
        "envs/tools.yaml"
    shell:
        "hmmalign --trim --outformat A2M {input.hmm} {input.fasta} > {output}"

rule hmmalign_trim:
    input:
        a2m = "analysis/phylogeny/{profile}.a2m",
        fai = "analysis/phylogeny/{profile}.a2m.seqkit.fai"
    output:
        "analysis/phylogeny/{profile}.a2m.trim"
    params:
        m = 300
    conda:
        "envs/tools.yaml"
    shell:
        "seqkit replace -sp [a-z] {input.a2m} | seqkit seq -nigm{params.m} | xargs seqkit faidx -f {input.a2m} | seqkit replace -sp [a-z] | seqkit rmdup -so {output}"

rule mafft_trimal:
    input:
        "{prefix}.mafft"
    output:
        "{prefix}.trimal"
    params:
        gt = 0.9
    conda:
        "envs/tools.yaml"
    shell:
        "trimal -in {input} -out {output} -gt {params.gt}"

rule algae_rmdup:
    input:
        "{prefix}.trimal"
    output:
        "{prefix}.trimal.uniq"
    conda:
        "envs/tools.yaml"
    shell:
        "seqkit rmdup -so {output} {input}"

rule algae_iqtree_hmm:
    input:
        "{prefix}.a2m.trim"
    output:
        "{prefix}.treefile"
    params:
        seed = 123
    threads:
        4
    conda:
        "envs/tools.yaml"
    shell:
        "iqtree2 -s {input} --prefix {wildcards.prefix} -redo --alrt 1000 -B 1000 --seed {params.seed} -T {threads}"

rule algae_ggtree_MCP_NCLDV:
    input:
        tree = "analysis/phylogeny/MCP_NCLDV.treefile",
        fasta_algae = expand("analysis/phylogeny/algae/hmm/{genome}-MCP_NCLDV.faa", genome = algal_genomes),
        fasta_PLVs  = expand("analysis/phylogeny/PLVs/hmm/{genome}-MCP_NCLDV.faa", genome = virus_names),
        blast_algae = expand("analysis/phylogeny/algae/blast/{genome}-MCP_NCLDV.blast", genome = algal_genomes),
        blast_PLVs  = expand("analysis/phylogeny/PLVs/blast/{genome}-MCP_NCLDV.blast", genome = virus_names),
        cp_queries  = "analysis/CPs/queries/MCP_NCLDV.faa",
        synonyms = "annotations/organisms.txt",
        neighnor_hmm = expand("hmm_algae/{hmm}.hmm", hmm = hmm_algae),
        neighbor_algae = expand("analysis/phylogeny/algae/neighbors/{genome}-{hmm}.tblout", genome = algal_genomes, hmm = hmm_algae),
        neighbor_PLVs = expand(expand("analysis/phylogeny/PLVs/neighbors/{genome}-{{hmm}}.tblout", genome = virus_names), hmm = hmm_algae),
        root_file = "annotations/MCP_NCLDV_root.txt"
    output:
        "analysis/phylogeny/MCP_NCLDV.svg"
    conda:
        "envs/r.yaml"
    script:
        "scripts/ggtree_collapse.R"

rule algae_ggtree_MCP_PLV:
    input:
        tree = "analysis/phylogeny/MCP_PLV.treefile",
        fasta_algae  = expand("analysis/phylogeny/algae/hmm/{genome}-{{CP}}.faa", genome = algal_genomes),
        fasta_PLVs   = expand("analysis/phylogeny/PLVs/hmm/{genome}-{{CP}}.faa", genome = virus_names),
        blast_algae  = expand("analysis/phylogeny/algae/blast/{genome}-{{CP}}.blast", genome = algal_genomes),
        blast_PLVs   = expand("analysis/phylogeny/PLVs/blast/{genome}-{{CP}}.blast", genome = virus_names),
        cp_queries   = "analysis/CPs/queries/{CP}.faa",
        # tblout = expand("analysis/hmm_algae/{genome}-{hmm}.tblout", genome = algal_genomes, hmm = hmm_algae),
        synonyms = "annotations/organisms.txt",
        hmm = expand("hmm_algae/{hmm}.hmm", hmm = hmm_algae),
        root_file = "annotations/MCP_PLV_root.txt"
    output:
        "analysis/phylogeny/{CP}.svg"
    conda:
        "envs/r.yaml"
    script:
        "scripts/ggtree_collapse.R"

rule algae_ggtree_MCP:
    input:
        tree = "analysis/phylogeny/MCP.treefile",
        fasta = expand("analysis/blast_algae/{genome}-MCP.faa", genome = algal_genomes),
        tblout = expand("analysis/hmm_algae/{genome}-{hmm}.tblout", genome = algal_genomes, hmm = hmm_algae),
        synonyms = "annotations/organisms.txt",
        hmm = expand("hmm_algae/{hmm}.hmm", hmm = hmm_algae)
    output:
        "analysis/phylogeny/MCP.svg"
    conda:
        "envs/r.yaml"
    script:
        "scripts/ggtree.R"

rule parse_hmmsearch:
    input:
        expand("analysis/hmmsearch/{{genome}}-{hmm}.txt", hmm = hmms)
    output:
        # figure = "images/segments_{genome}.pdf",
        bed = "analysis/segments/{genome}/segments.bed"
    params:
        e_value_threshold = 0.001,
        distance_threshold = 100000,
        genes_threshold = 2
    conda:
        "envs/r.yaml"
    script:
        "scripts/parse_hmmsearch.R"

rule segments_extract:
    input:
        genome = "genomes/{genome}.fna", bed = "analysis/segments/{genome}/segments.bed"
    output:
        "analysis/segments/{genome}/segments.fna"
    params:
        flanks = 50000
    conda:
        "envs/tools.yaml"
    shell:
        "seqkit subseq --up-stream {params.flanks} --down-stream {params.flanks} --bed {input.bed} {input.genome} | cut -f1 -d: > {output}"

checkpoint segments_split:
    input:
        "analysis/segments/{genome}/segments.fna"
    output:
        directory("analysis/segments/{genome}/segments_split")
    conda:
        "envs/tools.yaml"
    shell:
        """
        seqkit split -iO {output} {input}
        rename s/segments.id_// {output}/*.fna
        """

rule segments_genemarks:
    input:
        "analysis/segments/{genome}/segments_split/{segment}.fna"
    output:
        "analysis/segments/{genome}/segments_split/{segment}.genemarks.gff"
    shadow:
        "minimal"
    shell:
        "gmsn.pl --format GFF3 --virus --output {output} {input}"

rule segments_prodigal:
    input:
        "analysis/segments/{genome}/segments_split/{segment}.fna"
    output:
        "analysis/segments/{genome}/segments_split/{segment}.prodigal.gff"
    shadow:
        "minimal"
    conda:
        "envs/prokka.yaml"
    shell:
        "prodigal -i {input} -c -m -g 1 -p meta -f gff > {output}"

rule segments_genemarks_cat:
    input:
        "analysis/segments/{genome}/segments_split/{segment}.genemarks.gff",
        "analysis/segments/{genome}/segments_split/{segment}.prodigal.gff"
    output:
        "analysis/segments/{genome}/segments_split/{segment}.combined.gtf"
    params:
        outprefix = "analysis/segments/{genome}/segments_split/{segment}",
        cprefix = lambda wildcards:
            wildcards.genome.replace('-','') + '_' + sha256(wildcards.segment.encode('utf-8')).hexdigest()[0:6]
    conda:
        "envs/tools.yaml" # NB: conda gives 0.11.2
    shell:
        "gffcompare -p {params.cprefix} -CTo {params.outprefix} {input}"

rule segments_gffread:
    input:
        gtf = "analysis/segments/{genome}/segments_split/{segment}.combined.gtf",
        fna = "analysis/segments/{genome}/segments_split/{segment}.fna",
        fai = "analysis/segments/{genome}/segments_split/{segment}.fna.fai"
    output:
        gff = "analysis/segments/{genome}/segments_split/{segment}.gff",
        faa = "analysis/segments/{genome}/segments_split/{segment}.faa"
    conda:
        "envs/tools.yaml"
    shell:
        "gffread -g {input.fna} -w - -o {output.gff} {input.gtf} | seqkit translate --trim -o {output.faa}"

def aggregate_segments(wildcards, suffix):
    split_dir = checkpoints.segments_split.get(**wildcards).output[0]
    fna = path.join(split_dir, "{segment}.fna")
    segments ,= glob_wildcards(fna)
    files = path.join(split_dir, suffix)
    return expand(files, segment = segments)

rule segments_gff_cat:
    input:
        lambda wildcards: aggregate_segments(wildcards, "{segment}.gff")
    output:
        "analysis/segments/{genome}/segments.gff"
    shell:
        "cat {input} > {output}"

rule segments_faa_cat:
    input:
        lambda wildcards: aggregate_segments(wildcards, "{segment}.faa")
    output:
        "analysis/segments/{genome}/segments.faa"
    shell:
        "cat {input} > {output}"

rule segments_gvog:
    input:
        hmm = "databases/GVAll_proteins/output/GVOG.hmm",
        fasta = "analysis/segments/{genome}/segments.faa"
    output:
        tblout = "analysis/segments/{genome}/segments.gvog.tblout",
        domtblout = "analysis/segments/{genome}/segments.gvog.domtblout"
    threads:
        4
    conda:
        "envs/tools.yaml"
    shell:
        "hmmscan --cpu {threads} -o /dev/null --tblout {output.tblout} --domtblout {output.domtblout} {input.hmm} {input.fasta}"

rule segments_bellas:
    input:
        hmm = "databases/All_proteins_Sommaruga/output/PCs.hmmdb",
        fasta = "analysis/segments/{genome}/segments.faa"
    output:
        tblout = "analysis/segments/{genome}/segments.bellas.tblout",
        domtblout = "analysis/segments/{genome}/segments.bellas.domtblout"
    threads:
        4
    conda:
        "envs/tools.yaml"
    shell:
        "hmmscan --cpu {threads} -o /dev/null --tblout {output.tblout} --domtblout {output.domtblout} {input.hmm} {input.fasta}"

rule segments_vogdb:
    input:
        hmm = "databases/vogdb/output/vog.hmmdb",
        fasta = "analysis/segments/{genome}/segments.faa"
    output:
        tblout = "analysis/segments/{genome}/segments.vogdb.tblout",
        domtblout = "analysis/segments/{genome}/segments.vogdb.domtblout"
    threads:
        4
    conda:
        "envs/tools.yaml"
    shell:
        "hmmscan --cpu {threads} -o /dev/null --tblout {output.tblout} --domtblout {output.domtblout} {input.hmm} {input.fasta}"

rule segments_hmmsearch:
    input:
        fasta = "analysis/segments/{genome}/segments.faa", hmm = "hmm/{hmm}.hmm"
    output:
        "analysis/segments/{genome}/segments-hmm-{hmm}.tblout"
    conda:
        "envs/tools.yaml"
    shell:
        "hmmsearch -o /dev/null --tblout {output} {input.hmm} {input.fasta}"

rule segments_pfam:
    input:
        "analysis/segments/{genome}/segments.faa"
    output:
        "analysis/segments/{genome}/segments.pfam.txt"
    params:
        pfam_dir = "databases/Pfam"
    threads:
        4
    conda:
        "envs/bioperl.yaml"
    shell:
        "pfam_scan.pl -fasta {input} -dir {params.pfam_dir} -outfile {output} -cpu {threads}"

rule segments_blast:
    input:
        fna = "analysis/segments/{genome}/segments.fna",
        ndb = "analysis/segments/{genome}/segments.fna.ndb"
    output:
        "analysis/segments/{genome}/segments.fna.blastn"
    conda:
        "envs/tools.yaml"
    shell:
        "blastn -query {input.fna} -db {input.fna} -outfmt 6 -out {output} -evalue 1e-20"

rule segments_genbank:
    input:
        fna = "analysis/segments/{genome}/segments.fna",
        gff = "analysis/segments/{genome}/segments.gff",
        hmm_dbs = [ "databases/Pfam/Pfam-A.hmm" ] + expand("hmm/{hmm}.hmm", hmm = hmms),
        blastn = "analysis/segments/{genome}/segments.fna.blastn",
        hmmscan = [
            "analysis/segments/{genome}/segments.gvog.tblout",
            "analysis/segments/{genome}/segments.vogdb.tblout",
            "analysis/segments/{genome}/segments.bellas.tblout"
        ],
        markers = expand("analysis/segments/{{genome}}/segments-hmm-{hmm}.tblout", hmm = hmms),
        pfam = "analysis/segments/{genome}/segments.pfam.txt"
    output:
        "analysis/segments/{genome}/segments.gbk"
    params:
        coding_feature = "exon",
        evalue = 1e-5,
        min_repeat_len = 70,
        min_repeat_gap = 1000,
        min_repeat_id  = 90
    conda:
        "envs/biopython.yaml"
    script:
        "scripts/gff2gbk.py"

rule segments_list:
    input:
        lambda wildcards: aggregate_segments(wildcards, "{segment}.faa")
    output:
        "analysis/proteinortho/{genome}-segements.txt"
    shell:
        "ls {input} > {output}"

#rule proteinortho:
#    input:
#        expand("analysis/proteinortho/{genome}-segements.txt", genome = genomes)
#    output:
#        "analysis/proteinortho/viruses.proteinortho.tsv"
#    shadow:
#        "shallow"
#    threads:
#        30
#    params:
#        project = "viruses", dirname = "analysis/proteinortho"
#    shell:
#        """
#        cat {input} | xargs proteinortho -project={params.project} -cpus={threads} -p=ublast
#        mv {params.project}.* {params.dirname}/
#        """

rule collect_proteins:
    input:
        expand("analysis/PLVs/{genome}.faa", genome = small_viruses + Mesomimi_viruses),
        expand("analysis/PLV_segments/{segment}.faa", segment = segment_names)
    output:
        "analysis/hhblits_db/All_proteins.faa"
    conda:
        "envs/tools.yaml"
    shell:
        "seqkit seq -o {output} {input}"

rule segments_faa_ffindex:
    input:
        "analysis/PLVs/{segment}.faa"
    output:
        data  = "analysis/PLVs/{segment}.ffdata",
        index = "analysis/PLVs/{segment}.ffindex"
    conda:
        "envs/soedinglab.yaml"
    shell:
        "ffdb fasta -d {output.data} -i {output.index} {input}"

rule PLV_faa_ffindex:
    input:
        "analysis/PLV_segments/{segment}.faa"
    output:
        data  = "analysis/PLV_segments/{segment}.ffdata",
        index = "analysis/PLV_segments/{segment}.ffindex"
    conda:
        "envs/soedinglab.yaml"
    shell:
        "ffdb fasta -d {output.data} -i {output.index} {input}"

rule ffindex_num:
    input:
        "{prefix}.ffindex"
    output:
        "{prefix}.ffindex.num"
    shell:
        "cut -f2,3 {input} | nl -w1 > {output}"

rule hhblits_a3m:
    input:
        data  = "analysis/{dir}/{segment}.ffdata",
        index = "analysis/{dir}/{segment}.ffindex.num",
        msa_ffdata = "analysis/hhblits_db/All_proteins_msa_sequence.ffdata",
        msa_cs219_ffdata = "analysis/hhblits_db/All_proteins_msa_cs219.ffdata"
    output:
        data  = "analysis/{dir}_hh/{segment}-hhblits.a3m.ffdata",
        index = "analysis/{dir}_hh/{segment}-hhblits.a3m.ffindex"
    params:
        db = "analysis/hhblits_db/All_proteins_msa",
        n = 3,
        e = 1e-5
    conda:
        "envs/soedinglab.yaml"
    shell:
        "ffindex_apply -q -d {output.data} -i {output.index} {input.data} {input.index} -- hhblits -cpu 1 -v 0 -b 1 -z 1 -d {params.db} -i stdin -oa3m stdout -e {params.e} -n {params.n}"

rule hhblits_ss:
    input:
        data  = "analysis/{dir}_hh/{segment}-hhblits.a3m.ffdata",
        index = "analysis/{dir}_hh/{segment}-hhblits.a3m.ffindex"
    output:
        data  = "analysis/{dir}_hh/{segment}-hhblits-ss.a3m.ffdata",
        index = "analysis/{dir}_hh/{segment}-hhblits-ss.a3m.ffindex"
    conda:
        "envs/soedinglab.yaml"
    shell:
        "ffindex_apply -q -d {output.data} -i {output.index} {input.data} {input.index} -- $CONDA_PREFIX/scripts/addss.pl -v 0 /dev/stdin /dev/stdout"

rule hhsearch_self:
    input:
        data  = "analysis/{dir}_hh/{segment}-hhblits.a3m.ffdata",
        index = "analysis/{dir}_hh/{segment}-hhblits.a3m.ffindex",
        contxt = "databases/hhsuite/data/context_data.crf",
        msa_ffdata = "analysis/hhblits_db/All_proteins_msa_sequence.ffdata",
        msa_cs219_ffdata = "analysis/hhblits_db/All_proteins_msa_cs219.ffdata"
    output:
        data  = "analysis/{dir}_hh/{segment}-hhsearch-self.hhr.ffdata",
        index = "analysis/{dir}_hh/{segment}-hhsearch-self.hhr.ffindex"
    params:
        db = "analysis/hhblits_db/All_proteins_msa",
        BZ  = 250,
        p   = 20,
        ssm = 2
    conda:
        "envs/soedinglab.yaml"
    shell:
        "ffindex_apply -q -d {output.data} -i {output.index} {input.data} {input.index} -- hhsearch -v 1 -cpu 1 -b 1 -z 1 -i stdin -d {params.db} -o stdout -p {params.p} -Z {params.BZ} -B {params.BZ} -ssm {params.ssm} -contxt {input.contxt}"

rule hhsearch_pfam:
    input:
        data  = "analysis/{dir}_hh/{segment}-hhblits-ss.a3m.ffdata",
        index = "analysis/{dir}_hh/{segment}-hhblits-ss.a3m.ffindex",
        contxt = "databases/hhsuite/data/context_data.crf",
        db = "databases/hhsuite/data/pfam_hhm.ffdata"
    output:
        data  = "analysis/{dir}_hh/{segment}-hhsearch-pfam.hhr.ffdata",
        index = "analysis/{dir}_hh/{segment}-hhsearch-pfam.hhr.ffindex"
    params:
        db = "databases/hhsuite/data/pfam",
        BZ  = 250,
        p   = 20,
        ssm = 2
    conda:
        "envs/soedinglab.yaml"
    shell:
        "ffindex_apply -q -d {output.data} -i {output.index} {input.data} {input.index} -- hhsearch -v 1 -cpu 1 -b 1 -z 1 -i stdin -d {params.db} -o stdout -p {params.p} -Z {params.BZ} -B {params.BZ} -ssm {params.ssm} -contxt {input.contxt}"

rule hhsuite_parse:
    input:
        data  = "{prefix}.hhr.ffdata",
        index = "{prefix}.hhr.ffindex"
    output:
        "{prefix}.tsv"
    conda:
        "envs/soedinglab.yaml"
    shell:
        "ffindex_apply -q {input.data} {input.index} -- Rscript workflow/helpers/parse_hhsuite.R | sed '2,${{/^Query/d}}' > {output}"

rule vcontact2_cat:
    input:
        expand("analysis/PLV_segments/{segment}.faa", segment = PgVV_segments),
        expand("analysis/PLVs/{genome}.faa", genome = PgVV_viruses)
    output:
        "analysis/vcontact2/proteins.faa"
    shell:
        "cat {input} > {output}"

rule vcontact2_map:
    input:
        expand("analysis/PLV_segments/{segment}.faa", segment = PgVV_segments),
        expand("analysis/PLVs/{genome}.faa", genome = PgVV_viruses)
    output:
        "analysis/vcontact2/proteins.csv"
    conda:
        "envs/tools.yaml"
    shell:
        """
        parallel --tagstring {{/.}} seqkit seq -ni {{}} ::: {input} | awk -vOFS=, 'BEGIN{{print"protein_id","contig_id","keywords"}}{{print$2,$1,""}}' > {output}
        """

rule vcontact2:
    input:
        fasta = "analysis/vcontact2/proteins.faa",
        mapping = "analysis/vcontact2/proteins.csv"
    output:
        outdir = directory("analysis/vcontact2/results"),
        network  = "analysis/vcontact2/results/c1.ntw",
        genomes  = "analysis/vcontact2/results/genome_by_genome_overview.csv",
        profiles = "analysis/vcontact2/results/vConTACT_profiles.csv",
    conda:
        "envs/vcontact2.yaml"
    params:
        db = "None",
        rel_mode = "Diamond",
        pcs_mode = "MCL",
        vcs_mode = "ClusterONE"
    threads:
        workflow.cores
    shell:
        "vcontact2 --threads {threads} --db {params.db} --raw-proteins {input.fasta} --rel-mode {params.rel_mode} --proteins-fp {input.mapping} --pcs-mode {params.pcs_mode} --vcs-mode {params.vcs_mode} --c1-bin $CONDA_PREFIX/lib/cluster_one-v1.0.jar --output-dir {output.outdir}"

rule vcontact2_plot:
    input:
        network  = "analysis/vcontact2/results/c1.ntw",
        genomes  = "analysis/vcontact2/results/genome_by_genome_overview.csv",
        profiles = "analysis/vcontact2/results/vConTACT_profiles.csv",
        segments = "metadata/viral_segments.tsv",
        viruses  = "metadata/viruses.tsv"
    output:
        clustering = "output/vcontact2_clustering.svg"
    conda:
        "envs/r.yaml"
    script:
        "scripts/vcontact2.R"

rule protein_createdb_bellas:
    input:
        "databases/All_proteins_Sommaruga/input/All_proteins.faa"
    output:
        "analysis/hhblits_db/Bellas"
    conda:
        "envs/soedinglab.yaml"
    shell:
        "mmseqs createdb {input} {output}"

rule protein_createdb:
    input:
        "analysis/hhblits_db/All_proteins.faa"
    output:
        "analysis/hhblits_db/All_proteins"
    conda:
        "envs/soedinglab.yaml"
    shell:
        "mmseqs createdb {input} {output}"

rule protein_cluster:
    input:
        "analysis/hhblits_db/All_proteins"
    output:
        "analysis/hhblits_db/All_proteins_clu.0",
        "analysis/hhblits_db/All_proteins_clu.dbtype",
        "analysis/hhblits_db/All_proteins_clu.index"
    params:
        output_prefix = "analysis/hhblits_db/All_proteins_clu",
        tmp = "analysis/hhblits_db/tmp",
        min_seq_id = 0.3,
        coverage = 0.8
    threads:
        10
    conda:
        "envs/soedinglab.yaml"
    shell:
        "mmseqs cluster --min-seq-id {params.min_seq_id} -c {params.coverage} --threads {threads} {input} {params.output_prefix} {params.tmp}"

rule protein_result2msa:
    input:
        clu = "analysis/hhblits_db/All_proteins_clu.0",
        db = "analysis/hhblits_db/All_proteins"
    output:
        expand("analysis/hhblits_db/All_proteins_msa_{type}.{ext}", type = [ "ca3m", "header", "sequence" ], ext = [ "ffdata", "ffindex" ])
    params:
        input_prefix  = "analysis/hhblits_db/All_proteins_clu",
        output_prefix = "analysis/hhblits_db/All_proteins_msa"
    conda:
        "envs/soedinglab.yaml"
    shell:
        "mmseqs result2msa {input.db} {input.db} {params.input_prefix} {params.output_prefix} --msa-format-mode 1"

rule protein_cstranslate:
    input:
        msa_header_ffdata = "analysis/hhblits_db/All_proteins_msa_header.ffdata",
        db = "analysis/hhblits_db/All_proteins",
        cs219_lib = "databases/hhsuite/data/cs219.lib",
        context = "databases/hhsuite/data/context_data.lib"
    output:
        "analysis/hhblits_db/All_proteins_msa_cs219.ffdata",
        "analysis/hhblits_db/All_proteins_msa_cs219.ffindex"
    params:
        input_prefix  = "analysis/hhblits_db/All_proteins_msa",
        output_prefix = "analysis/hhblits_db/All_proteins_msa_cs219"
    threads:
        30
    # NB: conda doesn't have mpi version of hhsuite
    #conda:
    #    "envs/soedinglab.yaml"
    shell:
        "mpirun -np {threads} cstranslate_mpi -i {params.input_prefix} -o {params.output_prefix} -A {input.cs219_lib} -D {input.context} -x 0.3 -c 4 -I ca3m -b"

rule createtsv:
    input:
        db = "analysis/hhblits_db/All_proteins",
        clu = "analysis/hhblits_db/All_proteins_clu.0"
    output:
        "analysis/hhblits_db/All_proteins_clu.tsv"
    params:
        clu_prefix = "analysis/hhblits_db/All_proteins_clu"
    conda:
        "envs/soedinglab.yaml"
    shell:
        "mmseqs createtsv {input.db} {input.db} {params.clu_prefix} {output}"

rule mcl_abc:
    input:
        clu_tsv = "analysis/hhblits_db/All_proteins_clu.tsv",
        segment_tsv = expand("analysis/PLV_segments_hh/{segment}-hhsearch-self.tsv", segment = segment_names),
        virus_tsv = expand("analysis/PLVs_hh/{segment}-hhsearch-self.tsv", segment = small_viruses + Mesomimi_viruses),
    output:
        "analysis/hh_mcl/abc.tsv"
    params:
        coverage = 60,
        probab = 90
    conda:
        "envs/r.yaml"
    script:
        "scripts/abc_graph.R"

rule mcl_run:
    input:
        "analysis/hh_mcl/abc.tsv"
    output:
        "analysis/hh_mcl/mcl.txt"
    params:
        I = 1.5,
        scheme = 7
    conda:
        "envs/tools.yaml"
    threads:
        10
    shell:
        "mcl {input} -o {output} --abc -scheme {params.scheme} -te {threads} -I {params.I}"

rule mcl_analyze:
    input:
        "analysis/hh_mcl/mcl.txt",
        segment_index = expand("analysis/PLV_segments/{segment}.ffindex", segment = segment_names),
        virus_index   = expand("analysis/PLVs/{virus}.ffindex", virus = small_viruses + Mesomimi_viruses),
        segment_pfam  = expand("analysis/PLV_segments_hh/{segment}-hhsearch-pfam.tsv", segment = segment_names),
        virus_pfam    = expand("analysis/PLVs_hh/{virus}-hhsearch-pfam.tsv", virus = small_viruses + Mesomimi_viruses)
    output:
        "analysis/hh_mcl/mcl_genomes.txt"
    params:
        probab = 80
    conda:
        "envs/r.yaml"
    script:
        "scripts/mcl_graph.R"
