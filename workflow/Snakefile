
from pandas import read_csv, isnull
from os import path
from hashlib import sha256
from Bio import SeqIO
from collections import defaultdict

genomes,       = glob_wildcards("genomes/{genome}.fna")
algal_genomes, = glob_wildcards("genomes_algae/{genome}.fna")
hmms,          = glob_wildcards("hmm/{hmm}.hmm")
hmm_algae,     = glob_wildcards("hmm_algae/{hmm}.hmm")
capsid_proteins = defaultdict(list)

with open("metadata/capsid_proteins.txt") as fh:
    for line in fh:
        hmm, CP = line.rstrip().split()
        capsid_proteins[CP].append(hmm)

segments_file = 'metadata/viral_elements.tsv'
viruses_file  = 'metadata/viruses.tsv'

evalues = { "MCP_PLV": 1e-8, "MCP_NCLDV": 1e-8 }

# search = [ "autoblast", "blastp", "diamond", "usearch", "ublast", "lastp", "rapsearch", "topaz", "blatp", "mmseqsp" ]
search = [ "diamond" ]

segments = {}
data = read_csv(segments_file, sep = '\t')
for i, row in data.iterrows():
    if isnull(row['fragment']):
        short = row['short']
        segments[short] = row.to_dict()
segment_names = list(segments.keys())

viruses = {}
PgVV_group = []
data = read_csv(viruses_file, sep = '\t')
for i, row in data.iterrows():
    if isnull(row['fragment']):
        viruses[row['short']] = row.to_dict()
        if row['clade'] == 'PgVV':
            PgVV_group.append(row['short'])
virus_names = list(viruses.keys())

def orthology():
    prefix = []
    for short, row in viruses.items():
        if row['clade'] == "Mesomimi" or row['group'] == "small":
            prefix.append(short)
    return prefix

def mesomimi():
    prefix = []
    for short, row in viruses.items():
        if row['clade'] == "Mesomimi":
            prefix.append(short)
    return prefix

rule all:
    input:
        # expand("analysis/PLVs/orthogroups/{search}", search = search),
        # expand("analysis/hmmsearch/{genome}-{hmm}.txt", genome = genomes, hmm = hmms),
        # expand("analysis/hmmsearch/{genome}-{hmm}.faa", genome = genomes, hmm = hmms),
        expand("analysis/segments/{genome}/segments.gbk", genome = genomes),
        "analysis/vcontact2/results",
        # expand("analysis/blast_algae/{genome}.blast", genome = algal_genomes),
        # expand("analysis/phylogeny/{CP}.svg", CP = [ "MCP_NCLDV" ]),
        "analysis/promoters.svg",
        "output/vcontact2_clusrtring.svg"
        #"analysis/proteinortho/viruses.proteinortho.tsv",

rule viruses_ncbi_fna:
    output:
        "analysis/PLV_annotate/prokka-{genome}.fna"
    params:
        id = lambda w: viruses[w.genome]['accession']
    resources:
        ncbi = 1
    conda:
        "envs/tools.yaml"
    shell:
        "efetch -db nuccore -id {params.id} -format fasta > {output}"

rule viruses_ncbi_gbk:
    output:
        "analysis/PLV_annotate/ncbi-{genome}.gbk"
    params:
        id = lambda w: viruses[w.genome]['accession']
    resources:
        ncbi = 1
    conda:
        "envs/tools.yaml"
    shell:
        "efetch -db nuccore -id {params.id} -format gb > {output}"

rule extract_element:
    input:
        lambda w: expand("analysis/segments/{host}/segments.gbk", host = segments[w.segment]['genome'])
    output:
        "analysis/PLV_elements/{segment}.gbk"
    params:
        flank = 50000,
        segment = lambda w: segments[w.segment]
    conda:
        "envs/biopython.yaml"
    script:
        "scripts/extract_element.py"

rule mv_faa:
    input:
        lambda w: expand("analysis/PLV_annotate/{annotations}-{genome}.faa", annotations = viruses[w.genome]['annotations'], genome = w.genome)
    output:
        "analysis/PLVs/{genome}.faa"
    shell:
        "mv {input} {output}"

rule prokka_gbk:
    input:
        "analysis/PLV_annotate/prokka-{genome}.fna"
    output:
        "analysis/PLV_annotate/prokka-{genome}.gbk"
    params:
        outdir = "analysis/PLVs/ncbi_annotate/",
        locustag = lambda w: re.sub('[^A-Z]', '', w.genome.upper())
    threads:
        8
    conda:
        "envs/prokka.yaml"
    shell:
        """
        prokka --force --cpus {threads} --prefix {wildcards.genome} --locustag {params.locustag} --kingdom Viruses --outdir {params.outdir}/{wildcards.genome} {input}
        mv {params.outdir}/{wildcards.genome}/{wildcards.genome}.gbk {params.outdir}/
        """

rule flanks:
    input:
        "analysis/PLV_annotate/{genome}.gbk"
    output:
        "analysis/promoters/PLVs/{genome}.fna"
    params:
        min_len = 100,
        max_len = 150
    conda:
        "envs/bioperl.yaml"
    shell:
        "perl workflow/scripts/biotags.pl -i {input} -p CDS -t seq-{params.max_len} | awk '$1' | nl -w1 | seqkit tab2fx | seqkit seq -gm{params.min_len} -o {output}"

rule meme:
    input:
        "analysis/promoters/PLVs/{source}/{genome}.fna"
    output:
        "analysis/promoters/PLVs/{source}/{genome}/meme/meme.xml"
    params:
        outdir = "analysis/promoters/PLVs/{source}/{genome}/meme/",
        minw = 6,
        maxw = 16,
        nmotifs = 10,
        minsites = 10
    conda:
        "envs/tools.yaml"
    shell:
        "meme -minw {params.minw} -maxw {params.maxw} -nmotifs {params.nmotifs} -minsites {params.minsites} -maxsites Inf -dna -oc {params.outdir} {input}"

rule plot_meme:
    input:
        "analysis/promoters/PLVs/{source}/{genome}/meme/meme.xml"
    output:
        "analysis/promoters/PLVs/{source}/{genome}/meme/meme.svg"
    params:
        motif_res = [ "[ATW][ATW][ATW][ATW][ATW]TG[ATW]", "TCCGGA" ],
        meme_name = "{genome}"
    conda:
        "envs/r.yaml"
    script:
        "scripts/meme.R"

rule merge_meme:
    input:
        expand("analysis/promoters/PLVs/{prefix}/meme/meme.svg", prefix = mesomimi())
    output:
        "analysis/promoters.svg"
    shell:
        "python workflow/scripts/svg_stack.py --direction=v {input} > {output}"

rule extract_faa_bellas:
    input:
        "databases/Bellas_Sommaruga/input/All_proteins.faa"
    output:
        "analysis/PLV_annotate/bellas-{segment}.faa"
    params:
        search = lambda w: viruses[w.segment]['search']
    conda:
        "envs/tools.yaml"
    shell:
        "seqkit grep -rp {params.search} {input} | seqkit replace -p '$' -r ' {wildcards.segment}' -o {output}"

rule extract_gbk_yutin:
    input:
        "databases/Yutin2015.gbk"
    output:
        "analysis/PLV_annotate/yutin-{segment}.gbk"
    conda:
        "envs/tools.yaml"
    shell:
        "awk -vl={wildcards.segment} '/^LOCUS/{{p=$2==l}}p' {input} | sed -E 's/\\x93|\\x94/\"/g' > {output}"

rule extract_faa_yutin:
    input:
        "analysis/PLV_annotate/yutin-{segment}.gbk"
    output:
        "analysis/PLV_annotate/yutin-{segment}.faa"
    conda:
        "envs/bioperl.yaml"
    shell:
        "biotags.pl -i {input} -p CDS -T id -t locus_tag,translation | awk '{{printf\"%s %s\\t%s\\n\",$2,$1,$3}}' | seqkit tab2fx -o {output}"

rule extract_faa_ncbi_annotate:
    input:
        "analysis/PLV_annotate/prokka-{segment}.gbk"
    output:
        "analysis/PLV_annotate/prokka-{segment}.faa"
    conda:
        "envs/bioperl.yaml"
    shell:
        "biotags.pl -i {input} -p CDS -T description -t locus_tag,translation | awk -F\\\\t '$2{{printf\"%s %s\\t%s\\n\",$2,$1,$3}}' | seqkit tab2fx -o {output}"

rule extract_faa_ncbi:
    input:
        "analysis/PLV_annotate/ncbi-{segment}.gbk"
    output:
        "analysis/PLV_annotate/ncbi-{segment}.faa"
    conda:
        "envs/bioperl.yaml"
    shell:
        "biotags.pl -i {input} -p CDS -T description -t protein_id,translation | awk -F\\\\t '$2{{printf\"%s %s\\t%s\\n\",$2,$1,$3}}' | seqkit tab2fx -o {output}"

rule extract_faa_elements:
    input:
        "analysis/PLV_elements/{segment}.gbk"
    output:
        "analysis/PLV_elements/{segment}.faa"
    conda:
        "envs/bioperl.yaml"
    shell:
        "biotags.pl -i {input} -p CDS -t locus_tag,translation | awk '$1' | seqkit tab2fx -o {output}"

rule proteinortho:
    input:
        expand("analysis/PLV_elements/{segment}.faa", segment = segments.keys()),
        expand("analysis/PLVs/{prefix}.faa", prefix = orthology())
    output:
        "analysis/proteinortho/viruses.proteinortho.tsv"
    shadow:
        "shallow"
    params:
        evalue = 1e-5,
        conn   = 0.1,
        outdir = "analysis/proteinortho/",
        project = "viruses",
        search = "diamond"
    threads:
        workflow.cores
    conda:
        "envs/proteinortho.yaml"
    shell:
        """
        proteinortho -project={params.project} -p={params.search} -cpus={threads} -conn={params.conn} -e={params.evalue} {input}
        mv {params.project}.* {params.outdir}
        """

checkpoint proteinortho_collect:
    input:
        "analysis/PLVs/proteinortho/{search}/proteinortho.proteinortho.tsv",
        expand("analysis/PLVs/{genome}.faa", genome = virus_names)
    output:
        directory("analysis/PLVs/orthogroups/{search}")
    shadow:
        "shallow"
    conda:
        "envs/bioperl.yaml"
    shell:
        "perl workflow/scripts/proteinortho_grab_proteins.pl -t -s -output_dir={output} -output_file=sequences.faa -exact {input}"

rule getorf:
    input:
        "genomes/{genome}.fna"
    output:
        "analysis/getorf/{genome}.faa"
    conda:
        "envs/emboss.yaml"
    shell:
        "getorf -minsize 100 -filter {input} > {output}"

rule hmmsearch:
    input:
        fasta = "analysis/getorf/{genome}.faa", hmm = "hmm/{hmm}.hmm"
    output:
        "analysis/hmmsearch/{genome}-{hmm}.txt"
    conda:
        "envs/tools.yaml"
    shell:
        "hmmsearch -o /dev/null --tblout {output} {input.hmm} {input.fasta}"

rule extract_hmmsearch:
    input:
        fasta = "analysis/getorf/{genome}.faa",
        fai = "analysis/getorf/{genome}.faa.seqkit.fai",
        txt = "analysis/hmmsearch/{genome}-{hmm}.txt"
    output:
        "analysis/hmmsearch/{genome}-{hmm}.faa"
    conda:
        "envs/tools.yaml"
    shell:
        """
        awk '!/^#/&&$5<0.001{{print $1,$3}}' OFS=\\\\t {input.txt} | \
            parallel --colsep \\\\t seqkit faidx -f {input.fasta} {{1}} \| seqkit replace -p "'$'" -r '" {{2}}"' | seqkit  seqkit seq -gm 100
        """

rule algae_getorf:
    input:
        "genomes_algae/{genome}.fna"
    output:
        "analysis/getorf_algae/{genome}.faa"
    params:
        minsize = 100,
        maxsize = 100000
    conda:
        "envs/emboss.yaml"
    shell:
        "getorf -minsize {params.minsize} -maxsize {params.maxsize} -filter {input} > {output}"

rule makeblastdb_prot:
    input:
        "{fasta}"
    output:
        "{fasta}.pdb"
    conda:
        "envs/tools.yaml"
    shell:
        "makeblastdb -in {input} -dbtype prot"

rule makeblastdb_nucl:
    input:
        "{fasta}"
    output:
        "{fasta}.ndb"
    conda:
        "envs/tools.yaml"
    shell:
        "makeblastdb -in {input} -dbtype nucl"

rule extract_core_genes:
    input:
        tblout = "analysis/segments/{genome}/segments-hmm-{hmm}.tblout",
        fasta = "analysis/segments/{genome}/segments.faa",
        faidx = "analysis/segments/{genome}/segments.faa.fai"
    output:
        "analysis/CPs/core/{genome}-{hmm}.faa"
    params:
        evalue = 1e-5
    conda:
        "envs/tools.yaml"
    shell:
        "grep -v '^#' {input.tblout} | awk -ve={params.evalue} '$5<e' | cut -f1 -d' ' | xargs seqkit faidx -f {input.fasta} > {output}"

#rule combine_core_genes:
#    input:
#        fasta = lambda w: expand("analysis/CPs/core/{genome}-{hmm}.faa", genome = core_genomes, hmm = capsid_proteins[w.CP])
#    output:
#        "analysis/CPs/queries/{CP}.faa"
#    shell:
#        "cat {input} > {output}"

rule cluster_core_genes:
    input:
        "analysis/CPs/queries/{CP}.faa"
    output:
        "analysis/CPs/queries/{CP}.cdhit"
    params:
        c = 0.9
    conda:
        "envs/tools.yaml"
    shell:
        "cdhit -i {input} -o {output} -c {params.c} -d 0"

rule CP_align:
    input:
        "analysis/CPs/queries/{CP}.faa"
    output:
        "analysis/CPs/queries/{CP}.align"
    conda:
        "envs/tools.yaml"
    shell:
        "mafft {input} > {output}"

rule CP_hmmbuild:
    input:
        "analysis/CPs/queries/{CP}.align"
    output:
        "analysis/CPs/queries/{CP}.hmm"
    conda:
        "envs/tools.yaml"
    shell:
        "hmmbuild {output} {input}"

rule algae_hmmsearch:
    input:
        faa = "analysis/getorf_algae/{genome}.faa",
        hmm = "analysis/CPs/queries/{CP}.hmm"
    output:
        "analysis/phylogeny/algae/hmm/{genome}-{CP}.tblout"
    conda:
        "envs/tools.yaml"
    shell:
        "hmmsearch -o /dev/null --tblout {output} {input.hmm} {input.faa}"

rule PLV_hmmsearch:
    input:
        faa = "analysis/PLVs/{source}/{genome}.faa",
        hmm = "analysis/CPs/queries/{CP}.hmm"
    output:
        "analysis/phylogeny/PLVs/hmm/{source}/{genome}-{CP}.tblout"
    conda:
        "envs/tools.yaml"
    shell:
        "hmmsearch -o /dev/null --tblout {output} {input.hmm} {input.faa}"

rule algae_blast:
    input:
        faa = "analysis/getorf_algae/{genome}.faa",
        pdb = "analysis/getorf_algae/{genome}.faa.pdb",
        query = "analysis/CPs/queries/{CP}.faa"
    output:
        "analysis/phylogeny/algae/blast/{genome}-{CP}.blast"
    params:
        headers = "qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore stitle"
    conda:
        "envs/tools.yaml"
    shell:
        "blastp -query {input.query} -db {input.faa} -outfmt '6 {params.headers}' -out {output}"

rule PLV_blast:
    input:
        faa = "analysis/PLVs/{source}/{genome}.faa",
        pdb = "analysis/PLVs/{source}/{genome}.faa.pdb",
        query = "analysis/CPs/queries/{CP}.faa"
    output:
        "analysis/phylogeny/PLVs/blast/{source}/{genome}-{CP}.blast"
    params:
        headers = "qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore stitle"
    conda:
        "envs/tools.yaml"
    shell:
        "blastp -query {input.query} -db {input.faa} -outfmt '6 {params.headers}' -out {output}"

rule faidx:
    input:
        "{fasta}"
    output:
        "{fasta}.fai"
    conda:
        "envs/tools.yaml"
    shell:
        "seqkit faidx {input}"

rule faidx_f:
    input:
        "{fasta}"
    output:
        "{fasta}.seqkit.fai"
    conda:
        "envs/tools.yaml"
    shell:
        "seqkit faidx -f {input}"

rule hmmsearch_extract_algae:
    input:
        fasta = "analysis/getorf_algae/{genome}.faa",
        fai = "analysis/getorf_algae/{genome}.faa.seqkit.fai",
        tblout = "analysis/phylogeny/algae/hmm/{genome}-{CP}.tblout"
    output:
        "analysis/phylogeny/algae/hmm/{genome}-{CP}.faa"
    params:
        evalue = lambda w: evalues[w.CP]
    conda:
        "envs/tools.yaml"
    shell:
        "awk -ve={params.evalue} '$5<e' {input.tblout} | grep -v '^#' | cut -f1 -d' ' | sort -u | xargs -r seqkit faidx -f {input.fasta} > {output}"

rule hmmsearch_extract_PLV:
    input:
        fasta = "analysis/PLVs/{source}/{genome}.faa",
        fai = "analysis/PLVs/{source}/{genome}.faa.seqkit.fai",
        tblout = "analysis/phylogeny/PLVs/hmm/{source}/{genome}-{CP}.tblout"
    output:
        "analysis/phylogeny/PLVs/hmm/{source}/{genome}-{CP}.faa"
    params:
        evalue = lambda w: evalues[w.CP]
    conda:
        "envs/tools.yaml"
    shell:
        "awk -ve={params.evalue} '$5<e' {input.tblout} | grep -v '^#' | cut -f1 -d' ' | sort -u | xargs -r seqkit faidx -f {input.fasta} > {output}"

rule mad:
    input:
        "{treefile}"
    output:
        "{treefile}.rooted"
    shell:
        "mad {input}"

rule blast_extract_algae:
    input:
        fasta = "analysis/getorf_algae/{genome}.faa",
        fai = "analysis/getorf_algae/{genome}.faa.seqkit.fai",
        blast = "analysis/phylogeny/algae/blast/{genome}-{CP}.blast"
    output:
        "analysis/phylogeny/algae/blast/{genome}-{CP}.faa"
    params:
        evalue = 1e-10
    shell:
        "awk -ve={params.evalue} '$11<e' {input.blast} | cut -f2 | sort -u | xargs -r seqkit faidx -f {input.fasta} > {output}"

rule blast_extract_PLVs:
    input:
        fasta = "analysis/PLVs/{source}/{genome}.faa",
        fai = "analysis/PLVs/{source}/{genome}.faa.seqkit.fai",
        blast = "analysis/phylogeny/PLVs/blast/{source}/{genome}-{CP}.blast"
    output:
        "analysis/phylogeny/PLVs/blast/{source}/{genome}-{CP}.faa"
    params:
        evalue = 1e-10
    conda:
        "envs/tools.yaml"
    shell:
        "awk -ve={params.evalue} '$11<e' {input.blast} | cut -f2 | sort -u | xargs -r seqkit faidx -f {input.fasta} > {output}"

rule algae_hmm:
    input:
        fasta = "analysis/getorf_algae/{genome}.faa",
        hmm = "hmm_algae/{hmm}.hmm"
    output:
        "analysis/hmm_algae/{genome}-{hmm}.tblout"
    conda:
        "envs/tools.yaml"
    shell:
        "hmmsearch -o /dev/null --tblout {output} {input.hmm} {input.fasta}"


#rule blast_extract_cat:
#    input:
#        expand("analysis/phylogeny/algae/blast/{genome}-MCP_NCLDV.faa", genome = algal_genomes),
#        expand("analysis/phylogeny/PLVs/blast/{source}/{genome}-MCP_NCLDV.faa", zip, source = all_sources, genome = all_genomes)
#    output:
#        "analysis/phylogeny/MCP_NCLDV.fasta"
#    shell:
#        "seqkit seq -gm200 {input} -o {output}"

rule hmm_extract_cat:
    input:
        expand("analysis/phylogeny/algae/hmm/{genome}-{{CP}}.faa", genome = algal_genomes),
        expand("analysis/phylogeny/PLVs/hmm/{genome}-{{CP}}.faa", genome = virus_names)
    output:
        "analysis/phylogeny/{CP}.fasta"
    conda:
        "tools.yaml"
    shell:
        "seqkit seq -gm200 {input} -o {output}"

rule fasta_mafft:
    input:
        "{prefix}.fasta"
    output:
        "{prefix}.mafft"
    threads:
        20
    conda:
        "envs/tools.yaml"
    shell:
        "mafft --localpair --maxiterate 1000 --thread {threads} {input} > {output}"

rule fasta_hmmalign:
    input:
        fasta = "analysis/phylogeny/{profile}.fasta",
        hmm = "analysis/CPs/queries/{profile}.hmm"
    output:
        "analysis/phylogeny/{profile}.a2m"
    conda:
        "envs/tools.yaml"
    shell:
        "hmmalign --trim --outformat A2M {input.hmm} {input.fasta} > {output}"

rule hmmalign_trim:
    input:
        a2m = "analysis/phylogeny/{profile}.a2m",
        fai = "analysis/phylogeny/{profile}.a2m.seqkit.fai"
    output:
        "analysis/phylogeny/{profile}.a2m.trim"
    params:
        m = 300
    conda:
        "envs/tools.yaml"
    shell:
        "seqkit replace -sp [a-z] {input.a2m} | seqkit seq -nigm{params.m} | xargs seqkit faidx -f {input.a2m} | seqkit replace -sp [a-z] | seqkit rmdup -so {output}"

rule mafft_trimal:
    input:
        "{prefix}.mafft"
    output:
        "{prefix}.trimal"
    params:
        gt = 0.9
    conda:
        "envs/tools.yaml"
    shell:
        "trimal -in {input} -out {output} -gt {params.gt}"

rule algae_rmdup:
    input:
        "{prefix}.trimal"
    output:
        "{prefix}.trimal.uniq"
    conda:
        "envs/tools.yaml"
    shell:
        "seqkit rmdup -so {output} {input}"

rule algae_iqtree_hmm:
    input:
        "{prefix}.a2m.trim"
    output:
        "{prefix}.treefile"
    params:
        seed = 123
    threads:
        4
    conda:
        "envs/tools.yaml"
    shell:
        "iqtree2 -s {input} --prefix {wildcards.prefix} -redo --alrt 1000 -B 1000 --seed {params.seed} -T {threads}"

rule algae_ggtree_MCP_NCLDV:
    input:
        tree = "analysis/phylogeny/MCP_NCLDV.treefile",
        fasta_algae = expand("analysis/phylogeny/algae/hmm/{genome}-MCP_NCLDV.faa", genome = algal_genomes),
        fasta_PLVs  = expand("analysis/phylogeny/PLVs/hmm/{genome}-MCP_NCLDV.faa", genome = virus_names),
        blast_algae = expand("analysis/phylogeny/algae/blast/{genome}-MCP_NCLDV.blast", genome = algal_genomes),
        blast_PLVs  = expand("analysis/phylogeny/PLVs/blast/{genome}-MCP_NCLDV.blast", genome = virus_names),
        cp_queries  = "analysis/CPs/queries/MCP_NCLDV.faa",
        synonyms = "annotations/organisms.txt",
        neighnor_hmm = expand("hmm_algae/{hmm}.hmm", hmm = hmm_algae),
        neighbor_algae = expand("analysis/phylogeny/algae/neighbors/{genome}-{hmm}.tblout", genome = algal_genomes, hmm = hmm_algae),
        neighbor_PLVs = expand(expand("analysis/phylogeny/PLVs/neighbors/{genome}-{{hmm}}.tblout", genome = virus_names), hmm = hmm_algae),
        root_file = "annotations/MCP_NCLDV_root.txt"
    output:
        "analysis/phylogeny/MCP_NCLDV.svg"
    conda:
        "envs/r.yaml"
    script:
        "scripts/ggtree_collapse.R"

rule algae_ggtree_MCP_PLV:
    input:
        tree = "analysis/phylogeny/MCP_PLV.treefile",
        fasta_algae  = expand("analysis/phylogeny/algae/hmm/{genome}-{{CP}}.faa", genome = algal_genomes),
        fasta_PLVs   = expand("analysis/phylogeny/PLVs/hmm/{genome}-{{CP}}.faa", genome = virus_names),
        blast_algae  = expand("analysis/phylogeny/algae/blast/{genome}-{{CP}}.blast", genome = algal_genomes),
        blast_PLVs   = expand("analysis/phylogeny/PLVs/blast/{genome}-{{CP}}.blast", genome = virus_names),
        cp_queries   = "analysis/CPs/queries/{CP}.faa",
        # tblout = expand("analysis/hmm_algae/{genome}-{hmm}.tblout", genome = algal_genomes, hmm = hmm_algae),
        synonyms = "annotations/organisms.txt",
        hmm = expand("hmm_algae/{hmm}.hmm", hmm = hmm_algae),
        root_file = "annotations/MCP_PLV_root.txt"
    output:
        "analysis/phylogeny/{CP}.svg"
    conda:
        "envs/r.yaml"
    script:
        "scripts/ggtree_collapse.R"

rule algae_ggtree_MCP:
    input:
        tree = "analysis/phylogeny/MCP.treefile",
        fasta = expand("analysis/blast_algae/{genome}-MCP.faa", genome = algal_genomes),
        tblout = expand("analysis/hmm_algae/{genome}-{hmm}.tblout", genome = algal_genomes, hmm = hmm_algae),
        synonyms = "annotations/organisms.txt",
        hmm = expand("hmm_algae/{hmm}.hmm", hmm = hmm_algae)
    output:
        "analysis/phylogeny/MCP.svg"
    conda:
        "envs/r.yaml"
    script:
        "scripts/ggtree.R"

rule parse_hmmsearch:
    input:
        expand("analysis/hmmsearch/{{genome}}-{hmm}.txt", hmm = hmms)
    output:
        # figure = "images/segments_{genome}.pdf",
        bed = "analysis/segments/{genome}/segments.bed"
    params:
        e_value_threshold = 0.001,
        distance_threshold = 100000,
        genes_threshold = 2
    conda:
        "envs/r.yaml"
    script:
        "scripts/parse_hmmsearch.R"

rule segments_extract:
    input:
        genome = "genomes/{genome}.fna", bed = "analysis/segments/{genome}/segments.bed"
    output:
        "analysis/segments/{genome}/segments.fna"
    params:
        flanks = 50000
    conda:
        "envs/tools.yaml"
    shell:
        "seqkit subseq --up-stream {params.flanks} --down-stream {params.flanks} --bed {input.bed} {input.genome} | cut -f1 -d: > {output}"

checkpoint segments_split:
    input:
        "analysis/segments/{genome}/segments.fna"
    output:
        directory("analysis/segments/{genome}/segments_split")
    conda:
        "envs/tools.yaml"
    shell:
        """
        seqkit split -iO {output} {input}
        rename s/segments.id_// {output}/*.fna
        """

rule segments_genemarks:
    input:
        "analysis/segments/{genome}/segments_split/{segment}.fna"
    output:
        "analysis/segments/{genome}/segments_split/{segment}.genemarks.gff"
    shadow:
        "minimal"
    shell:
        "gmsn.pl --format GFF3 --virus --output {output} {input}"

rule segments_prodigal:
    input:
        "analysis/segments/{genome}/segments_split/{segment}.fna"
    output:
        "analysis/segments/{genome}/segments_split/{segment}.prodigal.gff"
    shadow:
        "minimal"
    conda:
        "envs/prokka.yaml"
    shell:
        "prodigal -i {input} -c -m -g 1 -p meta -f gff > {output}"

rule segments_genemarks_cat:
    input:
        "analysis/segments/{genome}/segments_split/{segment}.genemarks.gff",
        "analysis/segments/{genome}/segments_split/{segment}.prodigal.gff"
    output:
        "analysis/segments/{genome}/segments_split/{segment}.combined.gtf"
    params:
        outprefix = "analysis/segments/{genome}/segments_split/{segment}",
        cprefix = lambda wildcards:
            wildcards.genome.replace('-','') + '_' + sha256(wildcards.segment.encode('utf-8')).hexdigest()[0:6]
    conda:
        "envs/tools.yaml" # NB: conda gives 0.11.2
    shell:
        "gffcompare -p {params.cprefix} -CTo {params.outprefix} {input}"

rule segments_gffread:
    input:
        gtf = "analysis/segments/{genome}/segments_split/{segment}.combined.gtf",
        fna = "analysis/segments/{genome}/segments_split/{segment}.fna",
        fai = "analysis/segments/{genome}/segments_split/{segment}.fna.fai"
    output:
        gff = "analysis/segments/{genome}/segments_split/{segment}.gff",
        faa = "analysis/segments/{genome}/segments_split/{segment}.faa"
    conda:
        "envs/tools.yaml"
    shell:
        "gffread -g {input.fna} -w - -o {output.gff} {input.gtf} | seqkit translate --trim -o {output.faa}"

def aggregate_segments(wildcards, suffix):
    split_dir = checkpoints.segments_split.get(**wildcards).output[0]
    fna = path.join(split_dir, "{segment}.fna")
    segments ,= glob_wildcards(fna)
    files = path.join(split_dir, suffix)
    return expand(files, segment = segments)

rule segments_gff_cat:
    input:
        lambda wildcards: aggregate_segments(wildcards, "{segment}.gff")
    output:
        "analysis/segments/{genome}/segments.gff"
    shell:
        "cat {input} > {output}"

rule segments_faa_cat:
    input:
        lambda wildcards: aggregate_segments(wildcards, "{segment}.faa")
    output:
        "analysis/segments/{genome}/segments.faa"
    shell:
        "cat {input} > {output}"

rule segments_gvog:
    input:
        hmm = "databases/GVDB/output/GVOG.hmm",
        fasta = "analysis/segments/{genome}/segments.faa"
    output:
        tblout = "analysis/segments/{genome}/segments.gvog.tblout",
        domtblout = "analysis/segments/{genome}/segments.gvog.domtblout"
    threads:
        4
    conda:
        "envs/tools.yaml"
    shell:
        "hmmscan --cpu {threads} -o /dev/null --tblout {output.tblout} --domtblout {output.domtblout} {input.hmm} {input.fasta}"

rule segments_bellas:
    input:
        hmm = "databases/Bellas_Sommaruga/output/PCs.hmmdb",
        fasta = "analysis/segments/{genome}/segments.faa"
    output:
        tblout = "analysis/segments/{genome}/segments.bellas.tblout",
        domtblout = "analysis/segments/{genome}/segments.bellas.domtblout"
    threads:
        4
    conda:
        "envs/tools.yaml"
    shell:
        "hmmscan --cpu {threads} -o /dev/null --tblout {output.tblout} --domtblout {output.domtblout} {input.hmm} {input.fasta}"

rule segments_vogdb:
    input:
        hmm = "databases/vogdb/output/vog.hmmdb",
        fasta = "analysis/segments/{genome}/segments.faa"
    output:
        tblout = "analysis/segments/{genome}/segments.vogdb.tblout",
        domtblout = "analysis/segments/{genome}/segments.vogdb.domtblout"
    threads:
        4
    conda:
        "envs/tools.yaml"
    shell:
        "hmmscan --cpu {threads} -o /dev/null --tblout {output.tblout} --domtblout {output.domtblout} {input.hmm} {input.fasta}"

rule segments_hmmsearch:
    input:
        fasta = "analysis/segments/{genome}/segments.faa", hmm = "hmm/{hmm}.hmm"
    output:
        "analysis/segments/{genome}/segments-hmm-{hmm}.tblout"
    conda:
        "envs/tools.yaml"
    shell:
        "hmmsearch -o /dev/null --tblout {output} {input.hmm} {input.fasta}"

rule segments_pfam:
    input:
        "analysis/segments/{genome}/segments.faa"
    output:
        "analysis/segments/{genome}/segments.pfam.txt"
    params:
        pfam_dir = "databases/Pfam"
    threads:
        4
    conda:
        "envs/bioperl.yaml"
    shell:
        "pfam_scan.pl -fasta {input} -dir {params.pfam_dir} -outfile {output} -cpu {threads}"

rule segments_blast:
    input:
        fna = "analysis/segments/{genome}/segments.fna",
        ndb = "analysis/segments/{genome}/segments.fna.ndb"
    output:
        "analysis/segments/{genome}/segments.fna.blastn"
    conda:
        "envs/tools.yaml"
    shell:
        "blastn -query {input.fna} -db {input.fna} -outfmt 6 -out {output} -evalue 1e-20"

rule segments_genbank:
    input:
        fna = "analysis/segments/{genome}/segments.fna",
        gff = "analysis/segments/{genome}/segments.gff",
        hmm_dbs = [ "databases/Pfam/Pfam-A.hmm" ] + expand("hmm/{hmm}.hmm", hmm = hmms),
        blastn = "analysis/segments/{genome}/segments.fna.blastn",
        hmmscan = [
            "analysis/segments/{genome}/segments.gvog.tblout",
            "analysis/segments/{genome}/segments.vogdb.tblout",
            "analysis/segments/{genome}/segments.bellas.tblout"
        ],
        markers = expand("analysis/segments/{{genome}}/segments-hmm-{hmm}.tblout", hmm = hmms),
        pfam = "analysis/segments/{genome}/segments.pfam.txt"
    output:
        "analysis/segments/{genome}/segments.gbk"
    params:
        coding_feature = "exon",
        evalue = 1e-5,
        min_repeat_len = 70,
        min_repeat_gap = 1000,
        min_repeat_id  = 90
    conda:
        "envs/biopython.yaml"
    script:
        "scripts/gff2gbk.py"

rule segments_list:
    input:
        lambda wildcards: aggregate_segments(wildcards, "{segment}.faa")
    output:
        "analysis/proteinortho/{genome}-segements.txt"
    shell:
        "ls {input} > {output}"

#rule proteinortho:
#    input:
#        expand("analysis/proteinortho/{genome}-segements.txt", genome = genomes)
#    output:
#        "analysis/proteinortho/viruses.proteinortho.tsv"
#    shadow:
#        "shallow"
#    threads:
#        30
#    params:
#        project = "viruses", dirname = "analysis/proteinortho"
#    shell:
#        """
#        cat {input} | xargs proteinortho -project={params.project} -cpus={threads} -p=ublast
#        mv {params.project}.* {params.dirname}/
#        """

rule segments_faa_split:
    input:
        "analysis/segments/{genome}/segments.faa"
    output:
        directory("analysis/segments/{genome}/split")
    conda:
        "envs/tools.yaml"
    shell:
        "seqkit split -i -O {output} {input}"

rule segments_hhblits:
    input:
        "analysis/segments/{genome}/split"
    output:
        directory("analysis/segments/{genome}/hhblits")
    params:
        db = "databases/hhsuite/UniRef30_2020_06",
        n = 2,
        e = 1e-3,
        BZ = 250
    threads:
        workflow.cores
    shell:
        """
        find {input} -name '*.faa' | \
            parallel -j{threads} hhblits -cpu 1 -v 0 -b 1 -z 1 -i {{}} -d {params.db} -o /dev/null -oa3m stdout -e {params.e} -n {params.n} -B {params.BZ} -Z {params.BZ} \| \
            addss.pl -i stdin -o {output}/{{/.}}.a3m
        """

rule segments_hhsearch:
    input:
        "analysis/segments/{genome}/hhblits"
    output:
        directory("analysis/segments/{genome}/hhsearch")
    params:
        contxt = "databases/hhsuite/context_data.crf",
        db  = "vogdb/vog",
        BZ  = 250,
        p   = 20,
        ssm = 2
    threads:
        workflow.cores
    shell:
        """
        mkdir -p {output}
        find {input} -name '*.a3m' | \
            parallel -j{threads} hhsearch -cpu 1 -b 1 -z 1 -i {{}} -d {params.db} -o {output}/{{/.}}.hhr -p {params.p} -Z {params.BZ} -B {params.BZ} -ssm {params.ssm} -contxt {params.contxt}
        """

rule vcontact2_cat:
    input:
        expand("analysis/PLV_elements/{segment}.faa", segment = segments.keys()),
        expand("analysis/PLVs/{genome}.faa", genome = PgVV_group)
    output:
        "analysis/vcontact2/proteins.faa"
    shell:
        "cat {input} > {output}"

rule vcontact2_map:
    input:
        expand("analysis/PLV_elements/{segment}.faa", segment = segments.keys()),
        expand("analysis/PLVs/{genome}.faa", genome = PgVV_group)
    output:
        "analysis/vcontact2/proteins.csv"
    conda:
        "envs/tools.yaml"
    shell:
        """
        parallel --tagstring {{/.}} seqkit seq -ni {{}} ::: {input} | awk -vOFS=, 'BEGIN{{print"protein_id","contig_id","keywords"}}{{print$2,$1,""}}' > {output}
        """

rule vcontact2:
    input:
        fasta = "analysis/vcontact2/proteins.faa",
        mapping = "analysis/vcontact2/proteins.csv"
    output:
        outdir = directory("analysis/vcontact2/results"),
        network  = "analysis/vcontact2/results/c1.ntw",
        genomes  = "analysis/vcontact2/results/genome_by_genome_overview.csv",
        profiles = "analysis/vcontact2/results/vConTACT_profiles.csv",
    conda:
        "envs/vcontact2.yaml"
    params:
        db = "None",
        pcs_mode = "MCL",
        vcs_mode = "ClusterONE"
    threads:
        workflow.cores
    shell:
        "vcontact2 --threads {threads} --db {params.db} --raw-proteins {input.fasta} --rel-mode Diamond --proteins-fp {input.mapping} --pcs-mode {params.pcs_mode} --vcs-mode {params.vcs_mode} --c1-bin $CONDA_PREFIX/lib/cluster_one-v1.0.jar --output-dir {output.outdir}"

rule vcontact2_plot:
    input:
        network  = "analysis/vcontact2/results/c1.ntw",
        genomes  = "analysis/vcontact2/results/genome_by_genome_overview.csv",
        profiles = "analysis/vcontact2/results/vConTACT_profiles.csv",
        elements = "metadata/viral_elements.tsv",
        viruses  = "metadata/viruses.tsv"
    output:
        clustering = "output/vcontact2_clusrtring.svg"
    conda:
        "envs/r.yaml"
    script:
        "scripts/vcontact2.R"
